{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f838b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"unstructured[all-docs]\" pillow lxml pillow chromadb tiktoken langchain langchain-community python_dotenv langchain-google-genai langchain-openai google-generativeai ipykernel langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87190cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd/Share/daniyal/ragchatbot/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "file_path = 'Alexendra_Lopez_Resume.pdf'\n",
    "\n",
    "# Reference: https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "chunks = partition_pdf(\n",
    "    filename=file_path,\n",
    "    infer_table_structure=True,            # extract tables\n",
    "    strategy=\"hi_res\",                     # mandatory to infer tables\n",
    "\n",
    "    extract_image_block_types=[\"Image\"],   # Add 'Table' to list to extract image of tables\n",
    "    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64\n",
    "\n",
    "    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage\n",
    "\n",
    "    chunking_strategy=\"by_title\",          # or 'basic'\n",
    "    max_characters=10000,                  # defaults to 500\n",
    "    combine_text_under_n_chars=2000,       # defaults to 0\n",
    "    new_after_n_chars=6000,\n",
    "\n",
    "    # extract_images_in_pdf=True,          # deprecated\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66eb719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.CompositeElement'>\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([str(type(el)) for el in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8724076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Title at 0x731d262bc470>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d262bc260>,\n",
       " <unstructured.documents.elements.Title at 0x731d262bc740>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d262bc890>,\n",
       " <unstructured.documents.elements.Title at 0x731d262bc860>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731c807f2ab0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d26263740>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629f5f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629f860>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629fef0>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629fb60>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629f800>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629ffe0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629cb30>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629f350>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629f3b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629f020>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629e9f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629e180>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629e8d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629e780>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629e570>,\n",
       " <unstructured.documents.elements.Title at 0x731d2629e300>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629e090>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629de20>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629dbe0>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629d6d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629daf0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629d280>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d261d8b90>,\n",
       " <unstructured.documents.elements.ListItem at 0x731c80556150>,\n",
       " <unstructured.documents.elements.ListItem at 0x731c80554c80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629fd70>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x731d2629ecf0>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629eb40>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629e660>,\n",
       " <unstructured.documents.elements.ListItem at 0x731d2629dc10>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].metadata.orig_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85962ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m elements = \u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m.metadata.orig_elements\n\u001b[32m      2\u001b[39m chunk_images = [el \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m elements \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mImage\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(el))]\n\u001b[32m      3\u001b[39m chunk_images[\u001b[32m0\u001b[39m].to_dict()\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "elements = chunks[3].metadata.orig_elements\n",
    "chunk_images = [el for el in elements if 'Image' in str(type(el))]\n",
    "chunk_images[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc7a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate tables from texts\n",
    "tables = []\n",
    "texts = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    if \"Table\" in str(type(chunk)):\n",
    "        tables.append(chunk)\n",
    "\n",
    "    if \"CompositeElement\" in str(type((chunk))):\n",
    "        texts.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a219ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the images from the CompositeElement objects\n",
    "def get_images_base64(chunks):\n",
    "    images_b64 = []\n",
    "    for chunk in chunks:\n",
    "        if \"CompositeElement\" in str(type(chunk)):\n",
    "            chunk_els = chunk.metadata.orig_elements\n",
    "            for el in chunk_els:\n",
    "                if \"Image\" in str(type(el)):\n",
    "                    images_b64.append(el.metadata.image_base64)\n",
    "    return images_b64\n",
    "\n",
    "images = get_images_base64(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd68dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.CompositeElement at 0x731d263206e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x731d26355c40>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa3f556",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Display the image\u001b[39;00m\n\u001b[32m      8\u001b[39m     display(Image(data=image_data))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m display_base64_image(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_base64_image(base64_code):\n",
    "    # Decode the base64 string to binary\n",
    "    image_data = base64.b64decode(base64_code)\n",
    "    # Display the image\n",
    "    display(Image(data=image_data))\n",
    "\n",
    "display_base64_image(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fa35599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673e7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an assistant tasked with summarizing tables and text.\n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Respond only with the summary, no additional comment.\n",
    "Do not start your message by saying \"Here is a summary\" or anything like that.\n",
    "Just give the summary as it is.\n",
    "\n",
    "Table or text chunk:\n",
    "{content}\n",
    "\"\"\")\n",
    "\n",
    "# Model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0.3)\n",
    "\n",
    "# Chain\n",
    "summarize_chain = prompt | model | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6642a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [{\"content\": t} for t in texts]\n",
    "\n",
    "# Run batch with concurrency control\n",
    "text_summaries = summarize_chain.batch(inputs, config={\"max_concurrency\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c2eb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summarize tables\n",
    "tables_html = [{\"content\":table.metadata.text_as_html} for table in tables]\n",
    "table_summaries = summarize_chain.batch(tables_html, {\"max_concurrency\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f779173e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alexendra Lopez is a Senior AI Engineer with five years of experience in building and deploying scalable AI/ML systems, natural language processing, and deep learning pipelines. Her career progression includes roles from intern to senior engineer, where she has led teams, architected an LLM-based chatbot for over 50,000 users, optimized inference pipelines, and developed models for predictive analytics, computer vision, and NLP. She is skilled in the end-to-end machine learning lifecycle, including distributed training, model optimization for edge deployment, and creating scalable data pipelines.',\n",
       " \"A Stanford Data Science Master's and UC Berkeley Computer Science graduate specializing in NLP, LLMs, and Computer Vision. Proficient in Python, SQL, TensorFlow, and PyTorch, with experience across cloud platforms like AWS, GCP, and Azure. Key projects include building a real-time transcription system, an AI healthcare assistant for clinical note summarization, and an autonomous drone vision model.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901a948c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6a065ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Prompt\n",
    "prompt_template = \"\"\"Describe the image in detail. For context,\n",
    "the image is part of a research paper explaining the transformers architecture.\n",
    "Be specific about graphs, such as bar plots.\"\"\"\n",
    "\n",
    "# Build prompt messages\n",
    "messages = [\n",
    "    (\n",
    "        \"user\",\n",
    "        [\n",
    "            {\"type\": \"text\", \"text\": prompt_template},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": \"data:image/jpeg;base64,{image}\"},\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Gemini model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.3)\n",
    "\n",
    "# Chain\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "# Batch process images (list of base64 strings)\n",
    "inputs = [{\"image\": img} for img in images]\n",
    "image_summaries = chain.batch(inputs, config={\"max_concurrency\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a17e4026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The image depicts the architecture of a Transformer model, showcasing the flow of data and operations within both the encoder (left) and decoder (right) stacks.  Each stack is represented within a rounded rectangle.\\n\\n**Encoder (Left):**\\n\\n1. **Inputs:**  Raw input data feeds into the model.\\n2. **Input Embedding:** The input data is transformed into vector representations.\\n3. **Positional Encoding:** Positional information is added to the embeddings, as transformers don\\'t inherently understand sequence order. This is represented by a swirling symbol merging with the embedding output via a \\'+\\' symbol.\\n4. **N×:** This indicates that the following block is repeated N times, signifying multiple encoder layers.\\n5. **Multi-Head Attention:** This block performs self-attention, allowing the model to weigh the importance of different parts of the input sequence when encoding each word.\\n6. **Add & Norm:**  This represents a residual connection followed by layer normalization. The output of the multi-head attention is added to its input, and then layer normalization is applied.\\n7. **Feed Forward:** This is a fully connected feed-forward network applied to each position independently.\\n8. **Add & Norm:** Another residual connection and layer normalization after the feed-forward network.\\n\\n**Decoder (Right):**\\n\\n1. **Outputs (shifted right):** The output sequence, shifted one position to the right. This is a crucial detail for training, as it prevents the decoder from \"peeking\" at the next word during training.\\n2. **Output Embedding:**  Similar to the input embedding, this transforms the output sequence into vector representations.\\n3. **Positional Encoding:**  Similar to the encoder, positional encoding is added to the embeddings.\\n4. **N×:**  This block, like in the encoder, is repeated N times, representing multiple decoder layers.\\n5. **Masked Multi-Head Attention:**  Similar to the multi-head attention in the encoder, but with a mask applied. This mask prevents the decoder from attending to future positions in the output sequence during training, forcing it to predict the next word based only on the preceding words.\\n6. **Add & Norm:** Residual connection and layer normalization.\\n7. **Multi-Head Attention:** This attention mechanism takes the output of the encoder as key and value inputs, and the output of the masked multi-head attention as the query. This allows the decoder to attend to the relevant parts of the input sequence when generating the output.\\n8. **Add & Norm:** Residual connection and layer normalization.\\n9. **Feed Forward:**  A fully connected feed-forward network.\\n10. **Add & Norm:** Residual connection and layer normalization.\\n11. **Linear:** A linear transformation layer.\\n12. **Softmax:** A softmax layer to produce output probabilities over the vocabulary.\\n13. **Output Probabilities:** The final output of the model, representing the probability distribution over the vocabulary for the next word in the sequence.\\n\\nThe arrows indicate the flow of data through the model.  The connections looping back within each N× block represent the residual connections. The connections between the encoder and decoder represent the flow of information from the encoder to the decoder.  The different colors of the boxes likely represent different types of operations or layers.  The rounded rectangles encapsulating the N× repeated blocks visually separate the repeated layers from the other components of the architecture.',\n",
       " 'The image depicts a flowchart illustrating the scaled dot-product attention mechanism within the transformer architecture.  It shows the computational steps involved, arranged vertically from bottom to top.  Each step is represented by a rounded rectangle with the operation name inside.  Arrows indicate the flow of data between operations.\\n\\nAt the bottom, the inputs are labeled \"Q\" (Query), \"K\" (Key), and \"V\" (Value).  These inputs feed into the first \"MatMul\" (Matrix Multiplication) block, which is colored purple.\\n\\nThe output of the first MatMul operation flows upwards into a \"Scale\" block, colored yellow.  This implies the result of the matrix multiplication is then scaled.\\n\\nNext, the output of the \"Scale\" block goes into an optional \"Mask\" block, colored pink.  The \"(opt.)\" indicates that this step is not always applied, depending on the specific use case (e.g., masking future tokens during decoder operations).\\n\\nFollowing the \"Mask\" (or \"Scale\" if \"Mask\" is not used), the data flows into a \"SoftMax\" block, colored green. This applies the softmax function to the input.\\n\\nFinally, the output of the \"SoftMax\" operation goes into a second \"MatMul\" block (also purple), which also receives input directly from the initial \"V\" (Value) input from the bottom.  The output of this final MatMul operation is the output of the entire attention mechanism, indicated by the upward arrow at the top.  The direct connection from \"V\" to the final MatMul represents its role in calculating the weighted average based on the attention weights calculated in the preceding steps.',\n",
       " 'The image depicts a diagram of the scaled dot-product attention mechanism within a transformer architecture. It illustrates the data flow and operations involved.\\n\\nAt the bottom, there are three inputs represented by the letters V, K, and Q, which stand for Value, Key, and Query, respectively. Each input feeds into a separate \"Linear\" block. These blocks likely represent linear transformations (e.g., matrix multiplications).\\n\\nThe outputs of these three linear transformations are then passed as inputs to the central component, a purple rounded rectangle labeled \"Scaled Dot-Product Attention.\"  This block is depicted with multiple slightly offset layers, suggesting multiple attention heads are being used.\\n\\nThe output of the \"Scaled Dot-Product Attention\" block is then fed upwards into a yellowish rounded rectangle labeled \"Concat.\"  This indicates a concatenation operation, where the outputs of the multiple attention heads are combined.  Three slightly offset arrows going into the \"Concat\" block further emphasize the multiple inputs being combined.\\n\\nFinally, the output of the \"Concat\" block is passed into another gray rounded rectangle labeled \"Linear,\" representing a final linear transformation.  An upward arrow from this final \"Linear\" block indicates the output of the entire attention mechanism.\\n\\nAn arrow pointing to the right from the layered \"Scaled Dot-Product Attention\" block is labeled \"h,\" likely representing the number of attention heads.  The slightly offset, layered representation of the \"Scaled Dot-Product Attention\" block and the three arrows leading into the \"Concat\" block visually reinforce the concept of multiple attention heads being processed and then combined.  The use of different colors (purple, yellow, and gray) helps distinguish different stages of the process.  Black arrows clearly indicate the direction of data flow.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8b58999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_616543/2292279220.py:9: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"multi_modal_rag\",\n",
    "    embedding_function=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"),\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ee4473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error adding tables: Expected Embeddings to be non-empty list or numpy array, got [] in upsert.\n",
      "Error adding images: name 'image_summaries' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Add texts\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "    summary_texts = [\n",
    "        Document(page_content=summary, metadata={id_key: doc_ids[i]}) for i, summary in enumerate(text_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "except Exception as e:\n",
    "    print(f\"Error adding texts: {e}\")\n",
    "\n",
    "try:\n",
    "    # Add tables\n",
    "    table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "    summary_tables = [\n",
    "        Document(page_content=summary, metadata={id_key: table_ids[i]}) for i, summary in enumerate(table_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_tables)\n",
    "    retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "except Exception as e:\n",
    "    print(f\"Error adding tables: {e}\")\n",
    "\n",
    "try:\n",
    "    # Add image summaries\n",
    "    img_ids = [str(uuid.uuid4()) for _ in images]\n",
    "    summary_img = [\n",
    "        Document(page_content=summary, metadata={id_key: img_ids[i]}) for i, summary in enumerate(image_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_img)\n",
    "    retriever.docstore.mset(list(zip(img_ids, images)))\n",
    "except Exception as e:\n",
    "    print(f\"Error adding images: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50cac3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "docs = retriever.invoke(\n",
    "    \"name of candidate?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ba83842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education\n",
      "\n",
      "Master of Science in Data Science – Stanford University | 2017 – 2019 Bachelor of Science in Computer Science – University of California, Berkeley | 2013 – 2017\n",
      "\n",
      "Technical Skills\n",
      "\n",
      "Languages: Python, Java, C++, SQL, R ML/DL Frameworks: TensorFlow, PyTorch, Scikit-learn, Hugging Face Specializations: NLP, LLMs, Computer Vision, Reinforcement Learning Tools & Platforms: AWS, GCP, Azure, Docker, Kubernetes, MLflow, Ray Databases: PostgreSQL, MongoDB, BigQuery\n",
      "\n",
      "Projects\n",
      "\n",
      "* Multi-Language Real-Time Transcription System - Built streaming ASR pipeline with Whisper + WebRTC.\n",
      "\n",
      "*Al-Powered Healthcare Assistant - Deployed an NLP system for clinical notes summarization.\n",
      "\n",
      "«Autonomous Drone Vision Model - Created CNN models for real-time aerial object detection.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Alexendra Lopez\n",
      "\n",
      "San Francisco, CA | alexendra.lopez@email.com | (123) 456-7890 linkedin.com/in/alexendralopez | github.com/alexendralopez\n",
      "\n",
      "Professional Summary\n",
      "\n",
      "Senior AI Engineer with 5 years of progressive experience in building scalable AI/ML systems, natural language processing, and deep learning pipelines. Skilled in end-to-end ML lifecycle—from data preparation and model training to deployment in production. Adept at collaborating with cross-functional teams and mentoring junior engineers. Passionate about designing intelligent systems that deliver measurable business impact.\n",
      "\n",
      "Work Experience\n",
      "\n",
      "Senior AI Engineer – TechNova AI Solutions | San Francisco, CA\n",
      "\n",
      "Jan 2024 – Present\n",
      "\n",
      "«Leading a team of 4 engineers in developing real-time Al systems for large-scale enterprise\n",
      "\n",
      "applications.\n",
      "\n",
      "*Architected and deployed an LLM-based chatbot system serving 50k+ users monthly with 92% intent accuracy.\n",
      "\n",
      "* Optimized inference pipelines using ONNX and TensorRT, reducing latency by 40%.\n",
      "\n",
      "* Collaborating with product managers to define Al strategy for next-gen automation tools.\n",
      "\n",
      "AI Engineer – InnovaSoft | San Jose, CA\n",
      "\n",
      "Jul 2022 – Dec 2023\n",
      "\n",
      "* Designed ML models for predictive analytics in healthcare, improving early disease detection by 18%.\n",
      "\n",
      "«Implemented distributed training pipelines with PyTorch Lightning + Ray, cutting model training time by 50%.\n",
      "\n",
      "* Contributed to internal Al framework that standardized deployment across 6 business units.\n",
      "\n",
      "Machine Learning Engineer – NextGen Robotics | Palo Alto, CA\n",
      "\n",
      "Apr 2021 – Jun 2022\n",
      "\n",
      "* Built computer vision models for robotic perception, achieving 95% accuracy in object recognition.\n",
      "\n",
      "* Developed a real-time SLAM system integrating LIDAR and deep learning for autonomous navigation.\n",
      "\n",
      "«Worked with embedded systems team to optimize models for edge deployment.\n",
      "\n",
      "AI/ML Engineer – BrightData Analytics | Remote\n",
      "\n",
      "Oct 2020 – Mar 2021\n",
      "\n",
      "«Created scalable data pipelines for ingesting and processing 10M+ records daily.\n",
      "\n",
      "«Developed NLP models for customer feedback analysis across multiple languages.\n",
      "\n",
      "«Automated model monitoring and alerting systems for production environments.\n",
      "\n",
      "Junior Machine Learning Engineer – CloudSphere Technologies | Los Angeles, CA\n",
      "\n",
      "Aug 2019 – Sep 2020\n",
      "\n",
      "«Assisted in developing recommendation systems for e-commerce platforms.\n",
      "\n",
      "«Applied feature engineering and hyperparameter tuning to improve accuracy by 12%.\n",
      "\n",
      "* Deployed ML APIs using FastAPI and Docker.\n",
      "\n",
      "Machine Learning Intern – Visionary Labs | San Diego, CA\n",
      "\n",
      "Jan 2019 – Jul 2019\n",
      "\n",
      "* Built proof-of-concept image classification models using CNNs in TensorF low.\n",
      "\n",
      "* Conducted data preprocessing, augmentation, and model validation tasks.\n",
      "\n",
      "*Collaborated with senior engineers to present findings to stakeholders.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(str(doc) + \"\\n\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "980b0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from base64 import b64decode\n",
    "\n",
    "\n",
    "def parse_docs(docs):\n",
    "    \"\"\"Split base64-encoded images and texts\"\"\"\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception:\n",
    "            text.append(doc)\n",
    "    return {\"images\": b64, \"texts\": text}\n",
    "\n",
    "\n",
    "def build_prompt(kwargs):\n",
    "    docs_by_type = kwargs[\"context\"]\n",
    "    user_question = kwargs[\"question\"]\n",
    "\n",
    "    context_text = \"\"\n",
    "    if len(docs_by_type[\"texts\"]) > 0:\n",
    "        for text_element in docs_by_type[\"texts\"]:\n",
    "            context_text += text_element.text\n",
    "\n",
    "    # construct prompt with context (including images)\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based only on the following context, which can include text, tables, and the below image.\n",
    "    Context: {context_text}\n",
    "    Question: {user_question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt_content = [{\"type\": \"text\", \"text\": prompt_template}]\n",
    "\n",
    "    if len(docs_by_type[\"images\"]) > 0:\n",
    "        for image in docs_by_type[\"images\"]:\n",
    "            prompt_content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return ChatPromptTemplate.from_messages([HumanMessage(content=prompt_content)])\n",
    "\n",
    "\n",
    "# Use Gemini instead of OpenAI\n",
    "gemini_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.3)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(build_prompt)\n",
    "    | gemini_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain_with_sources = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(parse_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnablePassthrough().assign(\n",
    "        response=(\n",
    "            RunnableLambda(build_prompt)\n",
    "            | gemini_model\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "961b981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* **TechNova AI Solutions:** Senior AI Engineer\n",
      "* **InnovaSoft:** AI Engineer\n",
      "* **NextGen Robotics:** Machine Learning Engineer\n",
      "* **BrightData Analytics:** AI/ML Engineer\n",
      "* **CloudSphere Technologies:** Junior Machine Learning Engineer\n",
      "* **Visionary Labs:** Machine Learning Intern\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    \"name of companies the candidate worked with and their roles?\"\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "849d202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: * **TechNova AI Solutions:** Senior AI Engineer\n",
      "* **InnovaSoft:** AI Engineer\n",
      "* **NextGen Robotics:** Machine Learning Engineer\n",
      "* **BrightData Analytics:** AI/ML Engineer\n",
      "* **CloudSphere Technologies:** Junior Machine Learning Engineer\n",
      "* **Visionary Labs:** Machine Learning Intern\n",
      "\n",
      "\n",
      "Context:\n",
      "Education\n",
      "\n",
      "Master of Science in Data Science – Stanford University | 2017 – 2019 Bachelor of Science in Computer Science – University of California, Berkeley | 2013 – 2017\n",
      "\n",
      "Technical Skills\n",
      "\n",
      "Languages: Python, Java, C++, SQL, R ML/DL Frameworks: TensorFlow, PyTorch, Scikit-learn, Hugging Face Specializations: NLP, LLMs, Computer Vision, Reinforcement Learning Tools & Platforms: AWS, GCP, Azure, Docker, Kubernetes, MLflow, Ray Databases: PostgreSQL, MongoDB, BigQuery\n",
      "\n",
      "Projects\n",
      "\n",
      "* Multi-Language Real-Time Transcription System - Built streaming ASR pipeline with Whisper + WebRTC.\n",
      "\n",
      "*Al-Powered Healthcare Assistant - Deployed an NLP system for clinical notes summarization.\n",
      "\n",
      "«Autonomous Drone Vision Model - Created CNN models for real-time aerial object detection.\n",
      "Page number:  2\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Alexendra Lopez\n",
      "\n",
      "San Francisco, CA | alexendra.lopez@email.com | (123) 456-7890 linkedin.com/in/alexendralopez | github.com/alexendralopez\n",
      "\n",
      "Professional Summary\n",
      "\n",
      "Senior AI Engineer with 5 years of progressive experience in building scalable AI/ML systems, natural language processing, and deep learning pipelines. Skilled in end-to-end ML lifecycle—from data preparation and model training to deployment in production. Adept at collaborating with cross-functional teams and mentoring junior engineers. Passionate about designing intelligent systems that deliver measurable business impact.\n",
      "\n",
      "Work Experience\n",
      "\n",
      "Senior AI Engineer – TechNova AI Solutions | San Francisco, CA\n",
      "\n",
      "Jan 2024 – Present\n",
      "\n",
      "«Leading a team of 4 engineers in developing real-time Al systems for large-scale enterprise\n",
      "\n",
      "applications.\n",
      "\n",
      "*Architected and deployed an LLM-based chatbot system serving 50k+ users monthly with 92% intent accuracy.\n",
      "\n",
      "* Optimized inference pipelines using ONNX and TensorRT, reducing latency by 40%.\n",
      "\n",
      "* Collaborating with product managers to define Al strategy for next-gen automation tools.\n",
      "\n",
      "AI Engineer – InnovaSoft | San Jose, CA\n",
      "\n",
      "Jul 2022 – Dec 2023\n",
      "\n",
      "* Designed ML models for predictive analytics in healthcare, improving early disease detection by 18%.\n",
      "\n",
      "«Implemented distributed training pipelines with PyTorch Lightning + Ray, cutting model training time by 50%.\n",
      "\n",
      "* Contributed to internal Al framework that standardized deployment across 6 business units.\n",
      "\n",
      "Machine Learning Engineer – NextGen Robotics | Palo Alto, CA\n",
      "\n",
      "Apr 2021 – Jun 2022\n",
      "\n",
      "* Built computer vision models for robotic perception, achieving 95% accuracy in object recognition.\n",
      "\n",
      "* Developed a real-time SLAM system integrating LIDAR and deep learning for autonomous navigation.\n",
      "\n",
      "«Worked with embedded systems team to optimize models for edge deployment.\n",
      "\n",
      "AI/ML Engineer – BrightData Analytics | Remote\n",
      "\n",
      "Oct 2020 – Mar 2021\n",
      "\n",
      "«Created scalable data pipelines for ingesting and processing 10M+ records daily.\n",
      "\n",
      "«Developed NLP models for customer feedback analysis across multiple languages.\n",
      "\n",
      "«Automated model monitoring and alerting systems for production environments.\n",
      "\n",
      "Junior Machine Learning Engineer – CloudSphere Technologies | Los Angeles, CA\n",
      "\n",
      "Aug 2019 – Sep 2020\n",
      "\n",
      "«Assisted in developing recommendation systems for e-commerce platforms.\n",
      "\n",
      "«Applied feature engineering and hyperparameter tuning to improve accuracy by 12%.\n",
      "\n",
      "* Deployed ML APIs using FastAPI and Docker.\n",
      "\n",
      "Machine Learning Intern – Visionary Labs | San Diego, CA\n",
      "\n",
      "Jan 2019 – Jul 2019\n",
      "\n",
      "* Built proof-of-concept image classification models using CNNs in TensorF low.\n",
      "\n",
      "* Conducted data preprocessing, augmentation, and model validation tasks.\n",
      "\n",
      "*Collaborated with senior engineers to present findings to stakeholders.\n",
      "Page number:  1\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_sources.invoke(\n",
    "    \"name of companies the candidate worked with and their roles?\"\n",
    ")\n",
    "\n",
    "print(\"Response:\", response['response'])\n",
    "\n",
    "print(\"\\n\\nContext:\")\n",
    "for text in response['context']['texts']:\n",
    "    print(text.text)\n",
    "    print(\"Page number: \", text.metadata.page_number)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "for image in response['context']['images']:\n",
    "    display_base64_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dd447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
